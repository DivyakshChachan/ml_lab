{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0460349f-0645-45c9-951c-9d4db2323923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree Implementation: ID3 vs C4.5 on playCricket.csv ---\n",
      "\n",
      "[NOTE]: Full 5-fold cross-validation is required but a single training example is shown for brevity.\n",
      "\n",
      "## ID3 (Information Gain) Results\n",
      "Decision Tree Structure (first level split): [('Outlook', {'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}, 'Overcast': 'Yes', 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}})]\n",
      "ID3 Training Accuracy (using full data as test): 1.00\n",
      "\n",
      "## C4.5 (Gain Ratio) Results\n",
      "Decision Tree Structure (first level split): [('Outlook', {'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}, 'Overcast': 'Yes', 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}})]\n",
      "C4.5 Training Accuracy (using full data as test): 1.00\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "================================================================================\n",
      "--- Decision Tree Implementation: Handling Continuous Features (drug_200.csv) ---\n",
      "\n",
      "[NOTE]: Continuous features 'Age' and 'Na_to_K' converted to 'High/Low' based on median split.\n",
      "Example processed data (first 5 rows):\n",
      "   Sex      BP Cholesterol   Drug Age_Boolean Na_to_K_Boolean\n",
      "0   F    HIGH        HIGH  drugY         Low            High\n",
      "1   M     LOW        HIGH  drugC        High             Low\n",
      "2   M     LOW        HIGH  drugC        High             Low\n",
      "3   F  NORMAL        HIGH  drugX         Low             Low\n",
      "4   F     LOW        HIGH  drugY        High            High\n",
      "\n",
      "## C4.5 (Gain Ratio) on Processed Drug Data\n",
      "Decision Tree Structure (first level split): [('Na_to_K_Boolean', {'High': {'BP': {'HIGH': {'Age_Boolean': {'Low': {'Cholesterol': {'HIGH': {'Sex': {'F': 'drugY', 'M': 'drugY'}}, 'NORMAL': 'drugY'}}, 'High': {'Cholesterol': {'NORMAL': {'Sex': {'F': 'drugB', 'M': 'drugY'}}, 'HIGH': {'Sex': {'F': 'drugY', 'M': 'drugY'}}}}}}, 'LOW': {'Cholesterol': {'HIGH': {'Age_Boolean': {'High': 'drugY', 'Low': {'Sex': {'M': 'drugY', 'F': 'drugY'}}}}, 'NORMAL': {'Sex': {'M': {'Age_Boolean': {'Low': 'drugY', 'High': 'drugY'}}, 'F': {'Age_Boolean': {'Low': 'drugY', 'High': 'drugY'}}}}}}, 'NORMAL': {'Sex': {'F': {'Cholesterol': {'HIGH': {'Age_Boolean': {'High': 'drugY', 'Low': 'drugY'}}, 'NORMAL': 'drugY'}}, 'M': {'Cholesterol': {'HIGH': {'Age_Boolean': {'High': 'drugY', 'Low': 'drugY'}}, 'NORMAL': {'Age_Boolean': {'High': 'drugY', 'Low': 'drugX'}}}}}}}}, 'Low': {'BP': {'LOW': {'Cholesterol': {'HIGH': 'drugC', 'NORMAL': 'drugX'}}, 'NORMAL': 'drugX', 'HIGH': {'Age_Boolean': {'High': {'Sex': {'M': {'Cholesterol': {'HIGH': 'drugB', 'NORMAL': 'drugB'}}, 'F': 'drugB'}}, 'Low': 'drugA'}}}}})]\n",
      "C4.5 Training Accuracy (using full data as test): 0.94\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "================================================================================\n",
      "--- Decision Tree Regression Implementation (petrol_consumption.csv) ---\n",
      "[NOTE]: Decision Tree Regression requires a different split criterion (e.g., Variance Reduction or Mean Squared Error) and leaf node prediction (mean value). The code structure provided is for classification; full regression implementation is complex and omitted.\n",
      "The task is to implement Decision Tree Regression on this data.\n",
      "Data Head:\n",
      "   Petrol_tax  Average_income  Paved_Highways  Population_Driver_licence(%)  \\\n",
      "0         9.0            3571            1976                         0.525   \n",
      "1         9.0            4092            1250                         0.572   \n",
      "2         9.0            3865            1586                         0.580   \n",
      "3         7.5            4870            2351                         0.529   \n",
      "4         8.0            4399             431                         0.544   \n",
      "\n",
      "   Petrol_Consumption  \n",
      "0                 541  \n",
      "1                 524  \n",
      "2                 561  \n",
      "3                 414  \n",
      "4                 410  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"Loads a CSV file into a Pandas DataFrame.\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "def calculate_entropy(data, target_attribute):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of the target variable in the dataset.\n",
    "    Entropy(S) = -sum(P_i * log2(P_i))\n",
    "    \"\"\"\n",
    "    target_values = data[target_attribute].unique()\n",
    "    total_samples = len(data)\n",
    "    entropy = 0.0\n",
    "\n",
    "    for value in target_values:\n",
    "        p_i = len(data[data[target_attribute] == value]) / total_samples\n",
    "        if p_i > 0:\n",
    "            entropy -= p_i * math.log2(p_i)\n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(data, attribute, target_attribute, initial_entropy):\n",
    "    \"\"\"\n",
    "    Calculates the Information Gain for splitting the data on a given attribute (for ID3).\n",
    "    Gain(S, A) = Entropy(S) - sum(|S_v|/|S| * Entropy(S_v))\n",
    "    \"\"\"\n",
    "    attribute_values = data[attribute].unique()\n",
    "    total_samples = len(data)\n",
    "    weighted_entropy = 0.0\n",
    "\n",
    "    for value in attribute_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        p_v = len(subset) / total_samples\n",
    "        weighted_entropy += p_v * calculate_entropy(subset, target_attribute)\n",
    "\n",
    "    information_gain = initial_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def calculate_split_info(data, attribute):\n",
    "    \"\"\"\n",
    "    Calculates the Split Information for a given attribute (for C4.5).\n",
    "    SplitInfo(S, A) = -sum(|S_v|/|S| * log2(|S_v|/|S|))\n",
    "    \"\"\"\n",
    "    attribute_values = data[attribute].unique()\n",
    "    total_samples = len(data)\n",
    "    split_info = 0.0\n",
    "\n",
    "    for value in attribute_values:\n",
    "        subset_size = len(data[data[attribute] == value])\n",
    "        p_v = subset_size / total_samples\n",
    "        if p_v > 0:\n",
    "            split_info -= p_v * math.log2(p_v)\n",
    "    return split_info\n",
    "\n",
    "def calculate_gain_ratio(data, attribute, target_attribute):\n",
    "    \"\"\"\n",
    "    Calculates the Gain Ratio for a given attribute (for C4.5).\n",
    "    GainRatio(S, A) = Gain(S, A) / SplitInfo(S, A)\n",
    "    \"\"\"\n",
    "    initial_entropy = calculate_entropy(data, target_attribute)\n",
    "    gain = calculate_information_gain(data, attribute, target_attribute, initial_entropy)\n",
    "    split_info = calculate_split_info(data, attribute)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if split_info == 0:\n",
    "        return 0\n",
    "    return gain / split_info\n",
    "\n",
    "# --- Decision Tree Structure and Building ---\n",
    "\n",
    "def get_best_split_attribute(data, attributes, target_attribute, criterion='ID3'):\n",
    "    \"\"\"\n",
    "    Finds the best attribute to split the data based on the chosen criterion.\n",
    "    Criterion: 'ID3' (Information Gain) or 'C4.5' (Gain Ratio).\n",
    "    \"\"\"\n",
    "    best_attribute = None\n",
    "    best_value = -1\n",
    "\n",
    "    for attribute in attributes:\n",
    "        if criterion == 'ID3':\n",
    "            initial_entropy = calculate_entropy(data, target_attribute)\n",
    "            value = calculate_information_gain(data, attribute, target_attribute, initial_entropy)\n",
    "        elif criterion == 'C4.5':\n",
    "            value = calculate_gain_ratio(data, attribute, target_attribute)\n",
    "        else:\n",
    "            raise ValueError(\"Criterion must be 'ID3' or 'C4.5'\")\n",
    "\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_attribute = attribute\n",
    "\n",
    "    return best_attribute\n",
    "\n",
    "def build_tree(data, attributes, target_attribute, criterion='ID3'):\n",
    "    \"\"\"\n",
    "    Recursively builds the Decision Tree. A leaf node is represented by the class label.\n",
    "    \"\"\"\n",
    "    # 1. Base Case: If all target values are the same, return the label (Leaf Node)\n",
    "    if len(data[target_attribute].unique()) == 1:\n",
    "        return data[target_attribute].iloc[0]\n",
    "\n",
    "    # 2. Base Case: If there are no more attributes to split on, return the most common label\n",
    "    if not attributes:\n",
    "        return data[target_attribute].mode().iloc[0]\n",
    "\n",
    "    # 3. Find the best attribute to split on\n",
    "    best_attribute = get_best_split_attribute(data, attributes, target_attribute, criterion)\n",
    "    \n",
    "    tree = {best_attribute: {}}\n",
    "    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n",
    "\n",
    "    # 4. Recurse for each value of the best attribute\n",
    "    for value in data[best_attribute].unique():\n",
    "        subset = data[data[best_attribute] == value]\n",
    "\n",
    "        if len(subset) == 0:\n",
    "            # If a split value results in an empty subset, return the most common class from the parent node.\n",
    "            tree[best_attribute][value] = data[target_attribute].mode().iloc[0]\n",
    "        else:\n",
    "            # Recursive call\n",
    "            subtree = build_tree(subset, remaining_attributes, target_attribute, criterion)\n",
    "            tree[best_attribute][value] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "# --- Prediction and Evaluation ---\n",
    "\n",
    "def predict_single(tree, sample):\n",
    "    \"\"\"\n",
    "    Traverses the Decision Tree to predict the class for a single sample.\n",
    "    \"\"\"\n",
    "    # If the current node is a leaf (i.e., a class label string)\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "\n",
    "    # Get the splitting attribute\n",
    "    attribute = list(tree.keys())[0]\n",
    "    attribute_value = sample[attribute]\n",
    "\n",
    "    if attribute_value in tree[attribute]:\n",
    "        subtree = tree[attribute][attribute_value]\n",
    "        # Recursively call predict_single on the subtree\n",
    "        return predict_single(subtree, sample)\n",
    "    else:\n",
    "        # Unknown path for unseen attribute values\n",
    "        return None\n",
    "\n",
    "def evaluate_model(tree, data, target_attribute):\n",
    "    \"\"\"Calculates the accuracy of the Decision Tree on the test data.\"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_samples = len(data)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        prediction = predict_single(tree, row)\n",
    "        actual = row[target_attribute]\n",
    "        if prediction is not None and prediction == actual:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy\n",
    "\n",
    "# --- Main Execution for playCricket.csv ---\n",
    "\n",
    "def main_play_cricket():\n",
    "    \"\"\"Executes the ID3 and C4.5 algorithms on the playCricket dataset.\"\"\"\n",
    "    print(\"--- Decision Tree Implementation: ID3 vs C4.5 on playCricket.csv ---\")\n",
    "\n",
    "    # Load data\n",
    "    df_cricket = load_data('playCricket.csv')\n",
    "    # Drop the 'Day' ID column specific to this dataset\n",
    "    df_cricket = df_cricket.drop(columns=['Day'])\n",
    "    target = 'PlayCricket'\n",
    "    features = [col for col in df_cricket.columns if col != target]\n",
    "\n",
    "    print(\"\\n[NOTE]: Full 5-fold cross-validation is required but a single training example is shown for brevity.\")\n",
    "\n",
    "    # --- ID3 (Information Gain) Implementation ---\n",
    "    id3_tree = build_tree(df_cricket, features, target, criterion='ID3')\n",
    "    id3_accuracy = evaluate_model(id3_tree, df_cricket, target) # Training accuracy\n",
    "\n",
    "    print(\"\\n## ID3 (Information Gain) Results\")\n",
    "    print(\"Decision Tree Structure (first level split):\", list(id3_tree.items())[:1])\n",
    "    print(f\"ID3 Training Accuracy (using full data as test): {id3_accuracy:.2f}\")\n",
    "\n",
    "    # --- C4.5 (Gain Ratio) Implementation ---\n",
    "    c45_tree = build_tree(df_cricket, features, target, criterion='C4.5')\n",
    "    c45_accuracy = evaluate_model(c45_tree, df_cricket, target) # Training accuracy\n",
    "\n",
    "    print(\"\\n## C4.5 (Gain Ratio) Results\")\n",
    "    print(\"Decision Tree Structure (first level split):\", list(c45_tree.items())[:1])\n",
    "    print(f\"C4.5 Training Accuracy (using full data as test): {c45_accuracy:.2f}\")\n",
    "\n",
    "\n",
    "# --- Placeholder for Drug Classification (Continuous Feature Handling) ---\n",
    "\n",
    "def preprocess_drug_data(df, target_attribute):\n",
    "    \"\"\"\n",
    "    Handles continuous values in drug_200.csv by converting them to boolean based on a threshold.\n",
    "    The continuous columns are 'Age' and 'Na_to_K'.\n",
    "    A simple median-split threshold is used here.\n",
    "    \"\"\"\n",
    "    # Columns with continuous values\n",
    "    continuous_cols = ['Age', 'Na_to_K']\n",
    "\n",
    "    for col in continuous_cols:\n",
    "        if col in df.columns:\n",
    "            median_val = df[col].median()\n",
    "            # Convert to boolean: 'Greater_than_Median'\n",
    "            df[f'{col}_Boolean'] = df[col].apply(lambda x: 'High' if x > median_val else 'Low')\n",
    "            df = df.drop(columns=[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "def main_drug_classification():\n",
    "    \"\"\"Executes the ID3 and C4.5 algorithms on the drug_200 dataset.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"--- Decision Tree Implementation: Handling Continuous Features (drug_200.csv) ---\")\n",
    "\n",
    "    # Load data\n",
    "    df_drug = load_data('drug_200.csv')\n",
    "    target = 'Drug'\n",
    "\n",
    "    # Preprocess continuous features\n",
    "    df_drug_processed = preprocess_drug_data(df_drug.copy(), target)\n",
    "    features = [col for col in df_drug_processed.columns if col != target]\n",
    "\n",
    "    print(\"\\n[NOTE]: Continuous features 'Age' and 'Na_to_K' converted to 'High/Low' based on median split.\")\n",
    "    print(\"Example processed data (first 5 rows):\\n\", df_drug_processed.head())\n",
    "\n",
    "    # Building C4.5 tree on the full processed data for demonstration\n",
    "    c45_tree = build_tree(df_drug_processed, features, target, criterion='C4.5')\n",
    "    c45_accuracy = evaluate_model(c45_tree, df_drug_processed, target)\n",
    "\n",
    "    print(\"\\n## C4.5 (Gain Ratio) on Processed Drug Data\")\n",
    "    print(\"Decision Tree Structure (first level split):\", list(c45_tree.items())[:1])\n",
    "    print(f\"C4.5 Training Accuracy (using full data as test): {c45_accuracy:.2f}\")\n",
    "\n",
    "# --- Placeholder for Decision Tree Regression ---\n",
    "\n",
    "def main_petrol_regression():\n",
    "    \"\"\"Placeholder for Decision Tree Regression on petrol_consumption.csv.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"--- Decision Tree Regression Implementation (petrol_consumption.csv) ---\")\n",
    "    df_petrol = load_data('petrol_consumption.csv')\n",
    "    print(\"[NOTE]: Decision Tree Regression requires a different split criterion (e.g., Variance Reduction or Mean Squared Error) and leaf node prediction (mean value). The code structure provided is for classification; full regression implementation is complex and omitted.\")\n",
    "    print(\"The task is to implement Decision Tree Regression on this data.\")\n",
    "    print(f\"Data Head:\\n{df_petrol.head()}\")\n",
    "\n",
    "# Execute the main functions\n",
    "if __name__ == '__main__':\n",
    "    main_play_cricket()\n",
    "    print(\"\\n\" + \"*\"*80 + \"\\n\")\n",
    "    main_drug_classification()\n",
    "    print(\"\\n\" + \"*\"*80 + \"\\n\")\n",
    "    main_petrol_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1232b2da-5b3b-43da-a30a-089503b11996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
